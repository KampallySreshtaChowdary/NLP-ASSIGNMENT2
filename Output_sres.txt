roberta : robustli optim bert pretrain approach kampal sreshta chowdari , b160809c 1 . summari recent year wide rang research breakthrough field of nlp ( natur languag process ) machin translat , sentiment analysi , text summar , question answer system , dialogu system on . introduct pretrain languag model nlp push forward limit natur languag understand natur languag gener . mani research institut compani explor way sophist alreadi exist languag model . one research breakthrough improv alreadi exist bert pretrain model roberta . natur languag process model made vital advanc introduct pretrain method , howev comput expens train ha made replic fine-tun paramet tough . studi done facebook AI univers washington , research analyz train googl 's bert ( bidirect encod represent transform ) model identifi mani chang train procedur enhanc perform . bert train on combin book corpu english wikipedia , contain 16 GB of uncompress text . order enhanc perform , research specif use new , larger novel dataset , ccnew ( data english portion the common crawl new dataset . data contain sixti three million english news articl publish sept 2016 feb 2019 . ( 76 GB filter ) . ) [ 2 ] , in order to verifi use data pretrain enhanc perform downstream task . analysi comparison pretrain model optim model downstream task done use follow 3 benchmark . ( 1 ) glue gener languag understand evalu ( glue ) benchmark collect nine dataset . use evalu natur languag understand system . task frame either single-sent classif or sentence-pair classif task . ( 2 ) squad stanford question answer dataset ( squad ) provid paragraph context question . task answer question extract relev span context . ( 3 ) race read comprehens examin ( race ) task a large-scal read comprehens dataset twent eight thousand passag nearli hundr thousand question . date : 12th may 2020. result optim model , roberta ( robustli optim bert approach ) , match exceed score recent introduc xlnet [ 3 ] model on glue benchmark . [ 1 ] mnli , qnli , qqp , rte , sst , mrpc , cola , st , wnli nine individu task glue benchmark . crux paper facebook AI research team found bert wa significantli undertrain suggest improv instruct train , call roberta . roberta use 160 GB dataset instead 16 GB dataset origin use train bert . also train longer time , increas number iter initi 100k 300k upto 500k larger batch i.e : - 8,000 instead 256 within origin bert base model . larger byte-level bpe vocabulari 50,000 subword unit rather character-level bpe vocabulari 30,000 subword unit . roberta remov next sequenc predict ( nsp , a binari classif loss predict whether two segment follow the origin text . ) object train procedur wherea nsp loss known import factor train origin bert model . roberta dynam chang mask pattern appli train data . key achiev roberta exce bert individu task the gener languag understand evalu ( glue ) benchmark . also match the recent introduc xlnet model glue benchmark set new state art 4 9 individu task shown tabl . massiv pretrain languag framework like roberta leverag within the busi set wide rang downstream task , togeth dialogu system , question answer , document classif , etc . futur research area could on incorpor sophist multi-task finetun procedur . refer [ 1 ] yinhan liu luke zettlemoy omer levi myle ott naman goyal jingfei Du mike lewi veselin stoyanov mandar joshi , danqi chen . roberta : : robustli optim bert pretrain approach . 2019 . [ 2 ] sebastian nagel . ccnew dataset , 2016 . [ 3 ] yime yang jaim carbonel ruslan salakhutdinov zhilin yang , zihang dai quoc V Le . xlnet : gener autoregress pretrain languag understand . 2019 . roberta : robustli optim bert pretrain approach kampal sreshta chowdari , b160809c 1 . summari recent year wide rang research breakthrough field of nlp ( natur languag process ) machin translat , sentiment analysi , text summar , question answer system , dialogu system on . introduct pretrain languag model nlp push forward limit natur languag understand natur languag gener . mani research institut compani explor way sophist alreadi exist languag model . one research breakthrough improv alreadi exist bert pretrain model roberta . natur languag process model made vital advanc introduct pretrain method , howev comput expens train ha made replic fine-tun paramet tough . studi done facebook AI univers washington , research analyz train googl 's bert ( bidirect encod represent transform ) model identifi mani chang train procedur enhanc perform . bert train on combin book corpu english wikipedia , contain 16 GB of uncompress text . order enhanc perform , research specif use new , larger novel dataset , ccnew ( data english portion the common crawl new dataset . data contain sixti three million english news articl publish sept 2016 feb 2019 . ( 76 GB filter ) . ) [ 2 ] , in order to verifi use data pretrain enhanc perform downstream task . analysi comparison pretrain model optim model downstream task done use follow 3 benchmark . ( 1 ) glue gener languag understand evalu ( glue ) benchmark collect nine dataset . use evalu natur languag understand system . task frame either single-sent classif or sentence-pair classif task . ( 2 ) squad stanford question answer dataset ( squad ) provid paragraph context question . task answer question extract relev span context . ( 3 ) race read comprehens examin ( race ) task a large-scal read comprehens dataset twent eight thousand passag nearli hundr thousand question . date : 12th may 2020. result optim model , roberta ( robustli optim bert approach ) , match exceed score recent introduc xlnet [ 3 ] model on glue benchmark . [ 1 ] mnli , qnli , qqp , rte , sst , mrpc , cola , st , wnli nine individu task glue benchmark . crux paper facebook AI research team found bert wa significantli undertrain suggest improv instruct train , call roberta . roberta use 160 GB dataset instead 16 GB dataset origin use train bert . also train longer time , increas number iter initi 100k 300k upto 500k larger batch i.e : - 8,000 instead 256 within origin bert base model . larger byte-level bpe vocabulari 50,000 subword unit rather character-level bpe vocabulari 30,000 subword unit . roberta remov next sequenc predict ( nsp , a binari classif loss predict whether two segment follow the origin text . ) object train procedur wherea nsp loss known import factor train origin bert model . roberta dynam chang mask pattern appli train data . key achiev roberta exce bert individu task the gener languag understand evalu ( glue ) benchmark . also match the recent introduc xlnet model glue benchmark set new state art 4 9 individu task shown tabl . massiv pretrain languag framework like roberta leverag within the busi set wide rang downstream task , togeth dialogu system , question answer , document classif , etc . futur research area could on incorpor sophist multi-task finetun procedur . refer [ 1 ] yinhan liu luke zettlemoy omer levi myle ott naman goyal jingfei Du mike lewi veselin stoyanov mandar joshi , danqi chen . roberta : : robustli optim bert pretrain approach . 2019 . [ 2 ] sebastian nagel . ccnew dataset , 2016 . [ 3 ] yime yang jaim carbonel ruslan salakhutdinov zhilin yang , zihang dai quoc V Le . xlnet : gener autoregress pretrain languag understand . 2019 . 